# ğŸ¤– AI Realtime Chat Interface (WebSockets + Streaming LLM)

A real-time AI chat interface built with **React + TypeScript**, **Node.js**, **Socket.io**, and **Groq Llama 3.1** for streaming AI responses.  
Supports markdown rendering, dark/light theme, typing indicators, persistence, and more.


# Features
Real-time messaging via WebSockets âœ… 
AI streaming responses (token-by-token) âœ… 
Message persistence (MongoDB) âœ…
Typing indicator animation  âœ… 
Dark/Light theme toggle âœ… 
Markdown rendering for AI responses  âœ… 
Copy message to clipboard âœ… 
Clear chat functionality âœ…
Connection & Reconnection handling âœ…
Input disabled while Ai response  âœ…
Timestamps âœ…

# Total Time Spent- 12 hours


---

## ğŸ› ï¸ Tech Stack

### **Frontend**
- React + TypeScript
- Socket.io Client
- TailwindCSS
- React Markdown + Remark GFM
- React Toastify
- Lottie Animations

### **Backend**
- Node.js + Express + TypeScript
- Socket.io Server
- MongoDB + Mongoose
- Groq LLM (Llama 3.1 model)
- Server-Sent Events (SSE) token streaming

---

## ğŸš€ Getting Started

### **1ï¸âƒ£ Clone the Repository**

```sh
git clone https://github.com/your-username/ai-chat-interface.git
cd ai-chat-interface

2ï¸âƒ£ Setup Environment Variables

Create a .env file in both client and server folders.

Backend .env

PORT=5000
MONGODB_URL=your_mongodb_connection_string
GROQ_API_KEY=your_groq_key
FRONTEND_URL=http://localhost:5173

Frontend .env
VITE_BACKEND_URL=http://localhost:5000


3ï¸âƒ£ Install Dependencies

Backend 
cd backend
npm install

Frontend
cd frontend
npm install

4ï¸âƒ£ Run the Project

Start Backend
npm run dev

Start Frontend
npm run dev

